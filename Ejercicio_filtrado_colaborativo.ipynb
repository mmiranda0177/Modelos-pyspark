{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio : Filtrado Colaborativo con Pandas/numpy/pyspark.ml.recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` y `numpy` son librerías que reúnen muchas herramientas para manejo de dataframe, vectores y matrices, junto a ellas se utilizará la componente `ml.recommendation`\n",
    "\n",
    "Puedes ejecutar cada una de las celdas de código haciendo click en ellas y presionando `Shift + Enter`. \n",
    "\n",
    "También puedes editar cualquiera de estas celdas. Las celdas no son independientes. Es decir, sí importa el orden en el que las ejecutes, y cualquier cambio que hagas se reflejará en las celdas que ejecutes después.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrado Coalborativo\n",
    "\n",
    "El filtrado colaborativo es una técnica utilizada por los sistemas de recomendación para solventar los problemas derivados de la sobreinformación que los consumidores sufren en Internet. Esta tendencia crece cada día más, debido a su enorme funcionalidad son más los usuarios que se valen de esta herramienta en sus búsquedas.\n",
    "\n",
    "`Existen diferentes tipos de filtrado a la hora de establecer las recomendaciones, se pueden clasificar en cuatro:`\n",
    "\n",
    "- Filtrados basado en contenido: las recomendaciones se hacen según los contenidos que puedan gustar o interesar.\n",
    "\n",
    "- Filtrados demográficos: se realizan por las características de los usuarios (edad, sexo, estudios…).\n",
    "\n",
    "- Filtrados colaborativos: las recomendaciones están basadas en las búsquedas con votos positivos de usuarios similares.\n",
    "\n",
    "- Filtrados híbridos: mezclan los dos o tres de los filtrados anteriores para una mejor experiencia.\n",
    "\n",
    "\n",
    "`Hay dos enfoques distintos para emplear el filtrado colaborativo:`\n",
    "\n",
    "- El filtrado basado en la memoria calcula las similitudes entre los productos o los usuarios.\n",
    "\n",
    "- El filtrado basado en modelos intenta aprender el patrón subyacente que dicta cómo los usuarios califican o interactúan con los elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este Ejercicio se refuerza los conceptos de _aprendizaje supervisado_ y a mostrar cómo usar `ml.recommendation` que implementa el algoritmo de los mínimos cuadrados alternos (ALS) para entrenar los modelos de nuestro primer Filtrado Colaborativo.\n",
    "\n",
    "`Mínimos cuadrados` es una técnica de análisis numérico enmarcada dentro de la optimización matemática, en la que, dados un conjunto de pares ordenados —variable independiente, variable dependiente— y una familia de funciones, se intenta encontrar la función continua, dentro de dicha familia, que mejor se aproxime a los datos (un \"mejor ajuste\"), de acuerdo con el criterio de mínimo error cuadrático.\n",
    "\n",
    "Cargamos el dataset de películas que podemos obtener en el sitio: https://grouplens.org/datasets/movielens/\n",
    "\n",
    "![](http://spark-mooc.github.io/web-assets/images/cs110x/movie-camera.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea la instancia de spark, utilizando `findspark`, donde podemos comprobar la ruta de donde se carga el spark con `findspark.find()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"rating\")\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession \n",
    "sq = SQLContext(sc)\n",
    "spark = SparkSession(sc)\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los dos archivos proporcionados: `la locación depende de donde se tenga en el equipo` donde tenemos:\n",
    "\n",
    "Archivo movies:\n",
    "\n",
    "- `movieId`: identificación de la película\n",
    "\n",
    "- `title`: nombre de la película\n",
    "\n",
    "- `genres`: género de la película\n",
    "\n",
    "Archivo ratings:\n",
    "\n",
    "- `user_id`: identificación del usuario\n",
    "\n",
    "- `movieId`: identificación de la película\n",
    "\n",
    "- `rating`: es la votación para la película de un usuario, está en una escala de 1-10 donde 10 es mejor (solo calificaciones enteras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, FloatType, StructType, StructField, StringType\n",
    "schema = StructType([StructField(\"movieId\", IntegerType()),\n",
    "                     StructField(\"title\", StringType()),\n",
    "                     StructField(\"genres\", StringType())])\n",
    "df_movies = spark.read.csv('file:///C:/movies.csv', header=True, inferSchema = True, schema=schema);\n",
    "df_movies.show(3)\n",
    "df_movies.printSchema()\n",
    "\n",
    "from pyspark.sql.types import IntegerType, StructType, StructField, StringType\n",
    "schema_gender = StructType([StructField(\"user_id\", IntegerType()),\n",
    "                            StructField(\"movieId\", IntegerType()),\n",
    "                             StructField(\"rating\", FloatType())])\n",
    "df_rating = spark.read.csv('file:///C:/ratings.csv', header=True, inferSchema = True, schema=schema_gender);\n",
    "df_rating.show(3)\n",
    "df_rating.printSchema()\n",
    "\n",
    "ratingsCount = df_rating.count()\n",
    "moviesCount = df_movies.count()\n",
    "\n",
    "print('hay {0} clasificaciones y {1} películas en datasets.'.format(ratingsCount, moviesCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización:\n",
    "\n",
    "El método que se utilizará para normalizar es el `MinMaxScaler` de la componente `pyspark.ml.feature`. Este método estadisticamente realiza la normalización de la siguiente manera:\n",
    "\n",
    "`MinMaxScaler` transforma un conjunto de datos de filas de vectores, normalizando cada característica individualmente de modo que esté en el rango dado. Toma parámetros:\n",
    "\n",
    "1.- `min`: 0.0 por defecto. Límite inferior después de la transformación, compartido por todas las características.\n",
    "\n",
    "2.- `max`: 1.0 por defecto. Límite superior después de la transformación, compartido por todas las características.\n",
    "\n",
    "\n",
    "Nota: `Para utilizar los escalamientos debemos realizar las trasnformaciones de variables a vectores y uan vez escalado volvemos a dejar la variable como antes para continuar nuestra ejecución.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vector_features = VectorAssembler(inputCols=[\"rating\"], outputCol=\"features\")\n",
    "df_rating_vec = vector_features.transform(df_rating)\n",
    "print(df_rating_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "standardizer = MinMaxScaler(min=0.0, max=1.0,\n",
    "                              inputCol='features',\n",
    "                              outputCol='norm_rating')\n",
    "modelstd = standardizer.fit(df_rating_vec)\n",
    "df_rating_norm = modelstd.transform(df_rating_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "firstelement=f.udf(lambda v:float(v[0]),FloatType())\n",
    "df_rating_norm = df_rating_norm.withColumn(\"norm_rating\", firstelement(\"norm_rating\"))\n",
    "print(df_rating_norm.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graficamos la distribución de rating:\n",
    "\n",
    "Realizamos el gráfico mediante `violin plot` para ver la distribución de los 10.000 primeras marcas de rating\n",
    "\n",
    "Dato: El `violin plot` combina la funcionalidad del box plot -mostrando los cuartiles de la distribución- y la de la estimación de densidad kernel.\n",
    "\n",
    "https://seaborn.pydata.org/generated/seaborn.violinplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# Convert Spark dataframe to Pandas to plot data distribution\n",
    "pandas_df = df_rating_norm.limit(100000).toPandas()\n",
    "sns.violinplot([pandas_df.norm_rating])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set de entranamiento, validación y pruebas\n",
    "\n",
    "Para nuestro caso crearemos un set de entranamiento, de validación y de pruebas, donde:\n",
    "\n",
    "- Training set (DataFrame), se utilizará para entrenar el modelo\n",
    "\n",
    "- Validation set (DataFrame), se utilizará para elegir el mejor modelo, validando lo obtenido en el entrenamiento.\n",
    "\n",
    "- Test set (DataFrame), Se utilizará para ejecutar nuestro entrenamiento.\n",
    "\n",
    "Entonces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "(trainingDFT, testDF) = df_rating_norm.randomSplit([0.8, 0.2], seed=seed)\n",
    "(trainingDF, valDF) = trainingDFT.randomSplit([0.8, 0.2], seed=seed)\n",
    "\n",
    "print('Training: {0}, val: {1}, test: {2}'.format(trainingDF.count(), valDF.count(), testDF.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento\n",
    "\n",
    "Utilizaremos el filtrado colaborativo basado en modelos:\n",
    "\n",
    "![](https://courses.edx.org/c4x/BerkeleyX/CS100.1x/asset/Collaborative_filtering.gif)\n",
    "\n",
    "Para entrenar el modelo utilizaremos la componente pyspark.ml.recommendation import ALS, la definición de la misma en la figura:\n",
    "\n",
    "![](http://spark-mooc.github.io/web-assets/images/matrix_factorization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selección del modelo\n",
    "\n",
    "El proceso de selección del modelo se realizará a través del análisis de validación cruzada manual y en base a la componente `pyspark.ml.tuning import CrossValidator` (cross-validation) con ajuste automático de hiperparámetros. \n",
    "Este ajuste se hace definiendo los posibles valores de los hiperparámetros del modelo y ejecutando una búsqueda en rejilla (grid-search) sobre éstas, para comparar la respuesta de los modelos resultantes y finalmente obtener el óptimo. Los hiperparámetros del modelo ALS son:\n",
    "\n",
    "- rank = la cantidad de factores latentes en el modelo (4, 8 y 12 como valores seleccionados)\n",
    "- maxIter = el número máximo de iteraciones (valor predeterminado)\n",
    "- regParam = el parámetro de regularización (0.1, 0.05 y 0.01 como valores seleccionados)\n",
    "\n",
    "Utilizaremos como medida a mse (mean squared error), entonces el modelo de menor mse será el elegido.\n",
    "\n",
    "`Mean squared error (MSE):` En estadística, el error cuadrático medio (ECM) de un estimador mide el promedio de los errores al cuadrado, es decir, la diferencia entre el estimador y lo que se estima. El ECM es una función de riesgo, correspondiente al valor esperado de la pérdida del error al cuadrado o pérdida cuadrática. La diferencia se produce debido a la aleatoriedad o porque el estimador no tiene en cuenta la información que podría producir una estimación más precisa.\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/53ab02a5a1847aa3ff5c6eb69b4023bfb73655f5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import numpy as np\n",
    "\n",
    "# Let's initialize our ALS learner\n",
    "als = ALS()\n",
    "als.setNonnegative\n",
    "# Now we set the parameters for the method\n",
    "als.setMaxIter(5)\\\n",
    "   .setSeed(10)\\\n",
    "   .setUserCol(\"user_id\")\\\n",
    "   .setItemCol(\"movieId\")\\\n",
    "   .setRatingCol(\"norm_rating\")\\\n",
    "   .setPredictionCol(\"predictions\")\\\n",
    "   .setColdStartStrategy(\"drop\")\\\n",
    "   .setNonnegative=True\n",
    "\n",
    "regEval = RegressionEvaluator(predictionCol=\"predictions\", labelCol=\"norm_rating\", metricName=\"mse\")\n",
    "\n",
    "ranks = [4, 8, 12]\n",
    "errors = [0, 0, 0]\n",
    "models = np.matrix(np.zeros(shape=(3,3), dtype=list))\n",
    "regparams = [0.1, 0.05, 0.01]\n",
    "err = 0\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "for rank in ranks:\n",
    "  # Set the rank here:\n",
    "  als.setRank(rank)\n",
    "  reg = 0\n",
    "  for regparam in regparams:\n",
    "      als.setRegParam(regparam)\n",
    "      # Create the model with these parameters.\n",
    "      model = als.fit(trainingDF)\n",
    "      # Run the model to create a prediction. Predict against the validation_df.\n",
    "      predict_df = model.transform(valDF)\n",
    "\n",
    "      # Remove NaN values from prediction (due to SPARK-14489)\n",
    "      predicted_ratings_df = predict_df.filter(predict_df.predictions != float('nan'))\n",
    "\n",
    "      # Run the previously created RMSE evaluator, reg_eval, on the predicted_ratings_df DataFrame\n",
    "      error = regEval.evaluate(predicted_ratings_df)\n",
    "      errors[err] = error\n",
    "      models[err,reg] = model\n",
    "      print(\"For rank %s and Regularizaión %s the RMSE is %s\",(rank, regparam, error))\n",
    "      if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = err\n",
    "        best_regp = reg\n",
    "      reg += 1\n",
    "  err += 1\n",
    "\n",
    "als.setRank(ranks[best_rank])\n",
    "als.setRegParam(regparams[best_regp])\n",
    "print(\"The best model was trained with rank %s y regularización %s y error mínimo es %s\" % (ranks[best_rank], regparams[best_regp], min_error))\n",
    "my_model = models[best_rank,best_regp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "alscv = ALS()\n",
    "alscv.setMaxIter(5)\\\n",
    "     .setUserCol(\"user_id\")\\\n",
    "     .setItemCol(\"movieId\")\\\n",
    "     .setRatingCol(\"norm_rating\")\\\n",
    "     .setPredictionCol(\"predictions\")\\\n",
    "     .setColdStartStrategy(\"drop\")\\\n",
    "     .setNonnegative=True\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(alscv.regParam, [0.1, 0.05, 0.01]).addGrid(alscv.rank, [4, 8, 12]).build()\n",
    "\n",
    "modelEvaluator = RegressionEvaluator(predictionCol=\"predictions\", labelCol=\"norm_rating\", metricName=\"mse\")\n",
    "\n",
    "crossval = CrossValidator(estimator=alscv,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=modelEvaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "cvModel = crossval.fit(trainingDF)\n",
    "best_als_model = cvModel.bestModel\n",
    "predict_dfcv = best_als_model.transform(valDF)\n",
    "# Remove NaN values from prediction (due to SPARK-14489)\n",
    "predicted_ratings_df = predict_df.filter(predict_df.predictions != float('nan'))\n",
    "\n",
    "# Run the previously created RMSE evaluator, reg_eval, on the predicted_ratings_df DataFrame\n",
    "errorcv = modelEvaluator.evaluate(predicted_ratings_df)\n",
    "\n",
    "print(\"Best number of latent factors (rank parameter): \" + str(best_als_model.rank))\n",
    "print(\"Best value of regularization factor: \" + str(best_als_model._java_obj.parent().getRegParam()))\n",
    "print(\"Max Iterations: \" + str(best_als_model._java_obj.parent().getMaxIter()))\n",
    "print(\"Error mejor modelo: %s\",errorcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validación de cual es el mejor modelo\n",
    "\n",
    "if min_error < errorcv:\n",
    "    model_final = best_als_model\n",
    "    print(\"Mejor modelo de cross validation manual\",  model_final)\n",
    "else:\n",
    "    model_final = my_model\n",
    "    print(\"Mejor modelo de cross validation Componente\",  model_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de medidas\n",
    "\n",
    "Para validar efectividad del modelo también podemos obtener otras medidas que pueden ser útiles para comparar con la nuestra con el objetivo de validar la minimización del error en nuestro modelo, estas son:\n",
    "\n",
    "`Root Mean Square Error (RMSE):` es una forma estándar de medir el error de un modelo en la predicción de datos cuantitativos. Formalmente se define de la siguiente manera:\n",
    "\n",
    "![](https://miro.medium.com/max/966/1*lqDsPkfXPGen32Uem1PTNg.png)\n",
    "\n",
    "\n",
    "`Mean Absolute Error (MAE):` mide la magnitud promedio de los errores en un conjunto de predicciones, sin considerar su dirección. Es el promedio sobre la muestra de prueba de las diferencias absolutas entre la predicción y la observación real donde todas las diferencias individuales tienen el mismo peso.\n",
    "\n",
    "![](https://miro.medium.com/max/630/1*OVlFLnMwHDx08PHzqlBDag.gif)\n",
    "\n",
    "\n",
    "`r² metric:` en estadística, el coeficiente de determinación, denominado R² y pronunciado R cuadrado, es un estadístico usado en el contexto de un modelo estadístico cuyo principal propósito es predecir futuros resultados o probar una hipótesis. El coeficiente determina la calidad del modelo para replicar los resultados, y la proporción de variación de los resultados que puede explicarse por el modelo\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/999c9c8c995140c586aff2c9342301846011d44f)\n",
    "\n",
    "https://es.wikipedia.org/wiki/Coeficiente_de_determinaci%C3%B3n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dfT = model_final.transform(testDF)\n",
    "\n",
    "def medidas_evaluation(predictions):\n",
    "    # Model evaluation in test - ratings regression evaluation\n",
    "    print(\"Model evaluation on test data:\")\n",
    "    predictions = predictions.na.drop()\n",
    "    # RMSE\n",
    "    rmse_evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"norm_rating\", predictionCol=\"predictions\")\n",
    "    rmse = rmse_evaluator.evaluate(predictions)\n",
    "    print(\"Root-mean-square error (RMSE) = \" + str(rmse))\n",
    "    # R2\n",
    "    r2_evaluator = RegressionEvaluator(metricName=\"r2\", labelCol=\"norm_rating\", predictionCol=\"predictions\")\n",
    "    r2 = r2_evaluator.evaluate(predictions)\n",
    "    print(\"r² metric = \" + str(r2))\n",
    "    # MAE\n",
    "    mae_evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"norm_rating\", predictionCol=\"predictions\")\n",
    "    mae = mae_evaluator.evaluate(predictions)\n",
    "    print(\"Mean Absolute Error (MAE) = \" + str(mae))\n",
    "\n",
    "    return [rmse, r2, mae]\n",
    "\n",
    "random_test_eval = medidas_evaluation(predict_dfT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recomendaciones\n",
    "\n",
    "Finalmente con el modelo ya entrenado podemos validar las recomendaciones, para ello crearemos una función que nos permitirá realizar:\n",
    "\n",
    "`Recomendación por Usuarios`: Ser realizará una recomendación por una cantidad de usuarios.\n",
    "\n",
    "`Recomendación de películas para usuarios`: Se realizará una recomendación de película para un grupo de usuarios\n",
    "\n",
    "`Recomendación para un usuario en particular`: Se realizará una recomendación de películas para un usuario en particular\n",
    "\n",
    "`Recomendación de película en particular para usuarios`: Se realizará recomendación de 1 película en particular para usuarios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Mostrar recomendación es una función que retorna el despliegue de datos recomendados por identificación.\n",
    "##Variables de entrada son:\n",
    "## Modelo: Modelo entrenado\n",
    "## df_2: es el dataframe complemento, que no se entreno pero que nos entrega información.\n",
    "## ident: Es el identificador de lo que queremos desplegar (columna de la predicción)\n",
    "## Cantidad: Corresponde a la cantidad de datos que desplegará.\n",
    "## Cantidad_recomendacion: Corresponde a la cantidad de recomendaciones por dato.\n",
    "## tipo: Si la recomendación será por \"Usuario\" o por \"película\"\n",
    "## variable: Corresponde a un dato opcional, que se ingresa cuando queremos recomendar para un dato en particular o película \n",
    "##o usuario\n",
    "\n",
    "def mostrar_recomendacion(modelo, df_2, ident, cantidad, cantidad_recomendacion, tipo, variable = 0):\n",
    "    datodf = df_2.toPandas()\n",
    "    if tipo == 'Usuario':\n",
    "        arrayrecomend = modelo.recommendForAllUsers(cantidad_recomendacion)\n",
    "        arrayuser = modelo.recommendForAllUsers(cantidad_recomendacion).select(ident)\n",
    "        if variable > 0:\n",
    "            recomend = np.array(arrayrecomend.select('recommendations').where(arrayrecomend[ident] == variable).collect(), dtype=int)\n",
    "            dato = np.array(arrayuser.where(arrayuser[ident] == variable).collect(), dtype=int)\n",
    "        else:\n",
    "            recomend = np.array(arrayrecomend.select('recommendations').collect(), dtype=int)\n",
    "            dato = np.array(arrayuser.collect(), dtype=int)   \n",
    "        \n",
    "        recomend_data  = recomend[:, 0, :, 0]\n",
    "        matrizrecomendacion = np.append(dato,recomend_data, axis = 1)\n",
    "        #print(matrizrecomendacion)\n",
    "        nombre = 'Recomendacion'\n",
    "        recomendacion = pd.DataFrame({tipo: matrizrecomendacion[:cantidad, 0]})\n",
    "        for j in range(1, cantidad_recomendacion+1):\n",
    "            pelicula = []\n",
    "            for i in range(cantidad):\n",
    "                pelicula.append(datodf['title'][datodf['movieId']==matrizrecomendacion[i, j]].iloc[0])\n",
    "\n",
    "            recomendacion[nombre+str(j)] = pelicula\n",
    "    else:\n",
    "        arrayrecomend = modelo.recommendForAllItems(cantidad_recomendacion)\n",
    "        arrayuser = modelo.recommendForAllItems(cantidad_recomendacion).select(ident)\n",
    "        if variable > 0:\n",
    "            recomend = np.array(arrayrecomend.select('recommendations').where(arrayrecomend[ident] == variable).collect(), dtype=int)\n",
    "            dato = np.array(arrayuser.where(arrayuser[ident] == variable).collect(), dtype=int)\n",
    "        else:\n",
    "            recomend = np.array(arrayrecomend.select('recommendations').collect(), dtype=int)\n",
    "            dato = np.array(arrayuser.collect(), dtype=int)\n",
    "        \n",
    "        recomend_data  = recomend[:, 0, :, 0]\n",
    "        nombre = 'Recomendacion'\n",
    "        recomendacion = pd.DataFrame({tipo: dato[:cantidad, 0]})\n",
    "        init =0\n",
    "        for i in range(init, cantidad):\n",
    "            if i == init:\n",
    "                recomendacion[\"Nombre\"] = None\n",
    "            recomendacion[\"Nombre\"].loc[i] = datodf['title'][datodf['movieId']==int(dato[i])].iloc[0]\n",
    "            for j in range(cantidad_recomendacion):\n",
    "                if i == init:\n",
    "                    recomendacion[nombre+str(j+1)] = None\n",
    "                recomendacion[nombre+str(j+1)].loc[i] = recomend_data[i, j]\n",
    "   \n",
    "    return recomendacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Recomendación por Usuarios`, se realizará recomendación de 5 películas para 5 usuarios (se puede variar el número)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "despliegue = mostrar_recomendacion(model_final, df_movies, 'user_id', 5, 5, 'Usuario')\n",
    "display(despliegue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Recomendación de películas para usuarios`: Se realizarán recomendación de para 5 usuarios para cada película"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "despliegue2 = mostrar_recomendacion(model_final, df_movies, 'movieId', 3, 5, 'Pelicula')\n",
    "display(despliegue2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Recomendación para un usuario en particular`: Se recomendará 3 películas para el usuario 471"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "despliegue3 = mostrar_recomendacion(model_final, df_movies, 'user_id', 1, 5, 'Usuario', 471)\n",
    "display(despliegue3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Recomendación de película en particular para usuarios`: Se recomendará la película `Out Cold (2001)` para 4 usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "despliegue4 = mostrar_recomendacion(model_final, df_movies, 'movieId', 1, 5, 'Pelicula', 4900)\n",
    "display(despliegue4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "\n",
    "Los algoritmos de filtrado colaborativo nos ayudan a generar recomendaciones a personas de acuerdo a sus gustos o preferencias de personas con gustos similares. Esto es de vital importancia para el negocio, dado que podemos establecer con precisión lo que necesita un usuario y por consiguiente mayor posibilidad de captar una venta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "\n",
    "Filtrado Colaborativo definición: https://es.wikipedia.org/wiki/Filtrado_colaborativo\n",
    "\n",
    "Filtrado Colaborativo definición y contexto: https://www.iebschool.com/blog/filtrado-colaborativo-sirve-e-commerce/\n",
    "\n",
    "Filtrado Colaborativo con pyspark: https://medium.com/datos-y-ciencia/intro-als-pyspark-7de7f3ba3b0a\n",
    "\n",
    "Componente filtrado Colaborativo pyspark: http://spark.apache.org/docs/latest/ml-collaborative-filtering.html\n",
    "\n",
    "k-nereast Neightbord en python: https://www.aprendemachinelearning.com/clasificar-con-k-nearest-neighbor-ejemplo-en-python/\n",
    "\n",
    "k-nereast Neightbord aproximación en pyspark: https://spark.apache.org/docs/2.2.0/ml-features.html#approximate-nearest-neighbor-search\n",
    "\n",
    "Ejemplo: https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2799933550853697/2823893187441173/2202577924924539/latest.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

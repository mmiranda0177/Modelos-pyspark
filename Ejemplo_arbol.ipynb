{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio : Clasificación con scikit-learn/pyspark.ml.classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` (o `sklearn`) es una librería que reúne muchas herramientas para realizar Minería de Datos y _Aprendizaje de Máquinas_. Permite hacer clasificación, clustering, entre otras. Además, incluye varios datasets para aprender a usar la librería.\n",
    "\n",
    "En este Ejercicio se refuerz los conceptos de _aprendizaje supervisado_ y a mostrar cómo usar `sklearn` y `ml.classification` para entrenar nuestro primer clasificador.\n",
    "\n",
    "Puedes ejecutar cada una de las celdas de código haciendo click en ellas y presionando `Shift + Enter`. \n",
    "\n",
    "También puedes editar cualquiera de estas celdas. Las celdas no son independientes. Es decir, sí importa el orden en el que las ejecutes, y cualquier cambio que hagas se reflejará en las celdas que ejecutes después.\n",
    "\n",
    "---\n",
    "\n",
    "Cargamos el Iris Dataset que viene en `sklearn`. El dataset contiene "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "print(\"Atributos:\", iris.feature_names)\n",
    "print()\n",
    "print(\"5 primeras filas:\")\n",
    "print(iris.data[0:5])\n",
    "print(iris.target[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los atributos del dataset son el largo y el ancho del pétalo y sépalo de cada flor.\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2014/intro_supervised_learning/iris_petal_sepal_1.png\" alt=\"\" style=\"width: 1000px;\"/>\n",
    "\n",
    "\n",
    "Las están dadas por el campo `target`, y los distintos tipos de `target` en el campo `target_names`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.data.shape)\n",
    "print(iris.target.shape)\n",
    "print(\"target_names:\", iris.target_names)\n",
    "print()\n",
    "print(\"Valores de la columna\")\n",
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que los primeros valores de la columna `target` son ceros en vez del nombre de la especie. Lo que se usa comúnmente es _mapear_ (asignar) números a variables categóricas. En este caso, el 0 corresponde a la primera especie en  `target_names`, es decir, a _iris setosa_. El 1 corresponde a _iris versicolor_ y el 2 a _iris virginica_.\n",
    "\n",
    "Los datos que vienen en `sklearn` ya están listos para ser usados con los métodos de la librería. Si hubiésemos recibido los datos como una tabla, éstos se verían más o menos así:\n",
    "\n",
    "| sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | target          |\n",
    "|-------------------|------------------|-------------------|------------------|-----------------|\n",
    "| 5.1               | 3.5              | 1.4               | 0.2              | iris-setosa     |\n",
    "| 4.9               | 3                | 1.4               | 0.2              | iris-setosa     |\n",
    "| ...               | ...              | ...               | ...              | ...             |\n",
    "| 5                 | 2                | 3.5               | 1                | iris-versicolor |\n",
    "| 5.9               | 3                | 5.1               | 1.8              | iris-virginica  |\n",
    "\n",
    "\n",
    "Una tarea que se nos podría plantear sería determinar, dados los atributos de una flor, cuál es la especie a la que corresponde. Por ejemplo, ¿a cuál especie corresponde la flor con los siguientes atributos?\n",
    "\n",
    "| sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | target |\n",
    "|-------------------|------------------|-------------------|------------------|--------|\n",
    "| 4.8               | 3                | 1.4               | 0.1              | ???    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo los datos cargados vamos a crear la instancia de spark, utilizando `findspark`, donde podemos comprobar la ruta de donde se carga el spark con `findspark.find()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"C:\\Spark\\spark-3.0.0-preview2-bin-hadoop2.7\")\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"iris\")\n",
    "\n",
    "from pyspark.sql import SQLContext \n",
    "sq = SQLContext(sc)\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque no es necesario cargar en RDD los datos, lo hacemos para mostrar como podemos cambiar un tipo `pandas` a un tipo `rdd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(iris.data)\n",
    "rdd2 = sc.parallelize(iris.target)\n",
    "print(rdd.take(2))\n",
    "print(rdd2.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un dataframe en base a los rdd cargados, utilizamos la componente `Row`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "iris_data_df = rdd.map(lambda x: \\\n",
    "               Row(sepalo_largo=float(x[0]), sepalo_ancho=float(x[1]), petalo_largo=float(x[2]), petalo_ancho=float(x[3]))).toDF()\n",
    "\n",
    "iris_target_df = rdd2.map(lambda y: \\\n",
    "               Row(label=int(y))).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos la distribución de los `target`, para ver cuantas de cada planta hay en este estudio.\n",
    "Ahora necesitamos unir los dataset de `data` y `target` ejecutar el clasificador, entonces utilizamos la componente `pandas` para `concatenarlos` (`pandas.concat()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datos = iris_target_df.toPandas()\n",
    "datos.loc[:,'Total'] = 0\n",
    "print(datos.groupby(['label'],as_index=False).agg({\"Total\":\"count\"}))\n",
    "\n",
    "dataset= pd.concat([iris_data_df.toPandas(), iris_target_df.toPandas()], axis=1,)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de ejecutar el calsificados, debemos crear `dataframe` en pyspark con `sq.createDataFrame(dataset)`, donde `sq` corresponde al `SQLContext(sc)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_total = sq.createDataFrame(dataset)\n",
    "print(data_total.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos la data para lograr entrenar, esto porque la componente `DecisionTreeClassifier` solicita:\n",
    "1.- Una entrada de `features` que corresponde a un vector con todas las características que se quiere entrenar.\n",
    "2.- Una entrada de `label` que corresponde a un vector de `target` asociados al entrenamiento.\n",
    "\n",
    "Entonces,\n",
    "    para lograr el vector de `features` utilizamos la componente `VectorAssembler` para el set de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "data_input = [\"sepalo_largo\", \"sepalo_ancho\", \"petalo_largo\", \"petalo_ancho\"]\n",
    "#target_input =[\"resultado\"]\n",
    "assemblerInputs = data_input# + target_input\n",
    "vector_features = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "total_V = vector_features.transform(data_total)\n",
    "print(total_V)\n",
    "print(data_total.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuestro primer clasificador\n",
    "\n",
    "Vamos a usar un _árbol de decisión_ como nuestro primer clasificador. Un árbol de decisión para este problema puede verse como el de la siguiente imagen:\n",
    "\n",
    "![](https://sebastianraschka.com/images/blog/2014/intro_supervised_learning/decision_tree_1.png)\n",
    "\n",
    "Nota que podemos mirar cualquiera de los atributos primero, o no usar algún otro atributo, por ejemplo:\n",
    "\n",
    "![](https://www.ibm.com/developerworks/library/ba-predictive-analytics2/fig06.gif)\n",
    "\n",
    "En el último caso no usamos el `petal width` como atributo para el árbol. Y así, podemos tener muchos árboles distintos.\n",
    "\n",
    "El proceso de _entrenar un clasificador_ corresponde al proceso de —en este caso— encontrar las reglas del árbol que _mejor se adapten a nuestros datos_. Llamaremos al árbol resultante el _**modelo**_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create initial Decision Tree Model\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n",
    "dtModel = dt.fit(total_V)\n",
    "print(\"numNodes = \", dtModel.numNodes)\n",
    "print(\"depth = \", dtModel.depth)\n",
    "print(dtModel.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Cómo evaluamos nuestro modelo?¶\n",
    "\n",
    "¿Cómo sabemos qué tan bien le fue? Es decir, ¿logró aprender desde los datos cuáles eran las mejores reglas?\n",
    "\n",
    "Una forma de ver esto es usando el modelo para clasificar nuevas instancias de los datos.\n",
    "\n",
    "Sin embargo, no tenemos nuevas instancias, ya que entrenamos el clasificador con todos los datos disponibles. Si evaluamos nuestro clasificador con los datos de entrenamiento (es decir, los datos que usamos para entrenar el clasificador y generar un modelo), vamos a tener resultados sobre-optimistas, ya que el clasificador usó esos mismos datos para entrenar. Es como si fueras a dar una prueba y usaras la misma prueba con las respuestas para estudiar.\n",
    "\n",
    "Esto también nos entrega una pista sobre qué significa que un clasificador aprenda de los datos. Para que un modelo se considere bueno_, no basta con que clasifique correctamente los datos que usó para entrenar, sino que debe clasificar correctamente datos que _no ha visto antes. Esto es a lo que se llama la capacidad de generalización del modelo.\n",
    "\n",
    "Vamos a definir un par de conceptos antes de continuar:\n",
    "\n",
    "El conjunto de datos de entrenamiento, o training set, es el conjunto de datos que le damos al clasificador para que pueda encontrar las reglas o parámetros óptimos que le permitan predecir la clase de estos datos.\n",
    "\n",
    "El conjunto de datos de prueba, o test set, es el conjunto de datos sobre el cual vamos a evaluar el rendimiento de nuestro modelo. Estos datos se eligen antes de cualquier modificación o limpieza del dataset, y sólo se usan para evaluar el modelo entrenado.\n",
    "\n",
    "(Una vez seguros de que nuestro modelo funciona bien y queremos usarlo \"en producción\", podemos entrenar con todos los datos disponibles. No antes)\n",
    "\n",
    "El último punto es muy importante. Si por ejemplo, normalizamos los datos primero, y después separamos en training y test sets, estaremos \"contaminando\" nuestros datos de entrenamiento, dándoles información del test set y en cierta forma \"haciendo trampa\", afectando la capacidad de generalización del modelo resultante.\n",
    "\n",
    "Holdout\n",
    "Ahora vamos a tomar una muestra de los datos y separarlos en training set y test set, respectivamente. ¿Cómo determinamos esta muestra?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de ejecutar el calsificados, debemos separar nuestro set de datos en datos de entrenamiento y datos de testing, para ello volvemos a crear dataframe en pyspark con sq.createDataFrame(dataset), donde sq corresponde al SQLContext(sc).\n",
    "\n",
    "Aplicamos training_total.randomSplit(), considerando para nuestro entrenamiento un 70% de datos de entrenamiento y 30% de datos de testing, que podemos ir modificando en acuerdo a aumentar el accuracy del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_total = sq.createDataFrame(dataset)\n",
    "(trainingData, testData) = training_total.randomSplit([0.67, 0.33], seed=100)\n",
    "print(trainingData.count())\n",
    "print(testData.count())\n",
    "print(testData.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ejecutamos sólo con data de `entrenamiento`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "data_input = [\"sepalo_largo\", \"sepalo_ancho\", \"petalo_largo\", \"petalo_ancho\"]\n",
    "#target_input =[\"resultado\"]\n",
    "assemblerInputs = data_input# + target_input\n",
    "vector_featuresT = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "train_V = vector_featuresT.transform(trainingData)\n",
    "print(train_V)\n",
    "print(training_total.columns)\n",
    "test_V = vector_featuresT.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos el clasificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create initial Decision Tree Model\n",
    "dt_T = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n",
    "dtModel_T = dt.fit(train_V)\n",
    "print(\"numNodes = \", dtModel_T.numNodes)\n",
    "print(\"depth = \", dtModel_T.depth)\n",
    "print(dtModel_T.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar, predecimos usando las observaciones en el `test set` y contrastamos el resultado con el `target` correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dtModel.transform(test_V)\n",
    "predictions.select('sepalo_largo', 'sepalo_ancho', 'petalo_largo', 'petalo_ancho','label', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(\"Precisiòn = %g\" % (accuracy*100))\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el _accuracy_ es de un 97% aprox.. Esto significa que clasificó correctamente el 97% de los datos en `X_test`.\n",
    "\n",
    "Surgen dos preguntas a partir de esto:\n",
    "\n",
    "1. ¿Tuvimos suerte? Es decir, si hubiésemos elegido otra partición train/test, ¿obtendríamos resultados diferentes?\n",
    "2. ¿Qué pasa si las clases están desbalanceadas? ¿Cómo afecta al accuracy si tenemos, por ejemplo, 99% de una clase y 1% de otra?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "_¿Tuvimos suerte? Es decir, si hubiésemos elegido otra partición train/test, ¿obtendríamos resultados diferentes?_\n",
    "\n",
    "Cross-validation nos ayuda a disminuir el efecto del azar (pregunta 1). Por ejemplo, observa qué pasa si cambiamos la semilla aleatoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "data_inputCV = [\"sepalo_largo\", \"sepalo_ancho\", \"petalo_largo\", \"petalo_ancho\"]\n",
    "#target_input =[\"resultado\"]\n",
    "assemblerInputsCV = data_inputCV# + target_input\n",
    "vector_featuresCV = VectorAssembler(inputCols=assemblerInputsCV, outputCol=\"features\")\n",
    "train_CV = vector_featuresCV.transform(trainingData)\n",
    "print(train_CV)\n",
    "print(training_total.columns)\n",
    "test_CV = vector_featuresCV.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create initial Decision Tree Model\n",
    "dt_CV = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n",
    "dtModel_CV = dt_CV.fit(train_CV)\n",
    "print(\"numNodes = \", dtModel_CV.numNodes)\n",
    "print(\"depth = \", dtModel_CV.depth)\n",
    "print(dtModel_CV.toDebugString)\n",
    "\n",
    "#train_CV divide en \n",
    "#total 100 datos\n",
    "#train_CV de entrenamiento: del 1 al 80\n",
    "#train_CV de validación: del 81 al 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para disminuir el efecto, _cross-validation_ particiona los datos en $k$ partes iguales, entrena con $k-1$ partes, evalúa en la $k$-ésima, guarda el resultado, y vuelve a repetir el proceso con otras $k-1$ partes hasta haber recorrido todas las partes. Este proceso se llama $k$-fold cross-validation.\n",
    "\n",
    "Observa que esto implica que el clasificador se entrenará $k$ veces, lo cual puede ser costoso dependiendo del clasificador y de la cantidad de datos.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth, [3, 6, 9, 12])\n",
    "             .addGrid(dt.maxBins, [7, 40, 80, 100])\n",
    "             .build())\n",
    "\n",
    "evaluatorCV = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "cv = CrossValidator(estimator=dt_CV, estimatorParamMaps=paramGrid, evaluator=evaluatorCV, numFolds=5)\n",
    "cvModel = cv.fit(train_CV)\n",
    "print(\"numNodes = \", cvModel.bestModel.numNodes)\n",
    "print(\"depth = \", cvModel.bestModel.depth)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsCV = cvModel.transform(test_CV)\n",
    "print(evaluatorCV.evaluate(predictionsCV))\n",
    "predictionsCV.select('sepalo_largo', 'sepalo_ancho', 'petalo_largo', 'petalo_ancho','label', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyCV = evaluatorCV.evaluate(predictionsCV)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracyCV))\n",
    "print(\"Precisiòn = %g\" % (accuracyCV*100))\n",
    "evaluatorCV.evaluate(predictionsCV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de confusión, Precision y Recall\n",
    "\n",
    "La _matriz de confusión_ nos permite observar los errores del clasificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_temp = predictionsCV.select(\"label\").groupBy(\"label\")\\\n",
    "                        .count().sort('count', ascending=False).toPandas()\n",
    "class_temp = class_temp[\"label\"].values.tolist()\n",
    "class_names = map(int, class_temp)\n",
    "# # # print(class_name)\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = predictionsCV.select(\"label\")\n",
    "y_true = y_true.toPandas()\n",
    "\n",
    "y_pred = predictionsCV.select(\"prediction\")\n",
    "y_pred = y_pred.toPandas()\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz se interpeta de la siguiente forma:\n",
    "\n",
    "| iris-setosa  | iris-versicolor  | iris-virginica  | ← clasificado como / clase real ↓ |\n",
    "|:----:|:----:|:----:|--------------------:|\n",
    "| 12 | 0  |   |              **iris-setosa** |\n",
    "| 0  | 17  | 1  |              **iris-versicolor** |\n",
    "| 0  | 2 | 18 |              **iris-virginica** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por cada clase, podemos determinar el tipo de error que el modelo hace.\n",
    "\n",
    "- **Verdaderos Positivos (TP)**: el dato X es de la clase C, y el modelo clasifica X como C.\n",
    "- **Verdaderos Negativos (TN)**: el dato X no es de la clase C, y el modelo clasifica X como algo que no es C.\n",
    "- **Falsos Positivos (FP)**: el dato X no es de la clase C, pero el modelo clasifica a X como C.\n",
    "- **Falsos Negativos (FN)**: el dato X es de la clase C, pero el modelo clasifica a X como algo que no es C.\n",
    "\n",
    "A partir de estas medidas, definimos dos medidas nuevas para una clase, _precision_ y _recall_:\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "$$Recall = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e2e427ec6dcf2d7882c3bbdc659a8204cba59dcc)\n",
    "\n",
    "Nota que estas medidas son para una clase en particular. La medida para todo el dataset puede ser el promedio de la medida para cada clase.\n",
    "\n",
    "Ver más en https://en.wikipedia.org/wiki/Precision_and_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=list(class_names),\n",
    "                      title='Confusion matrix, without normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=list(class_names), normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En conclusión\n",
    "\n",
    "El flujo usual a la hora de entrenar un clasificador es el siguiente:\n",
    "\n",
    "1. Tener datos. Verificar la fuente de los datos, la existencia de sesgos (sesgo de selección, sesgo del superviviente, sesgos sociodemográficos, etc.).\n",
    "2. Separar datos en train y test set.\n",
    "3. Realizar exploración y limpieza de datos en ambos sets, de manera independiente.\n",
    "4. Elegir clasificadores apropiados para el dominio del problema (próxima clase de cátedra)\n",
    "5. Determinar métricas de entrenamiento usando cross-validation, si es posible (más de esto en el lab de mañana).\n",
    "6. Evaluar en el test set.\n",
    "7. Usar todos los datos para entrenar el modelo que irá \"a producción\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "\n",
    "1. Documentación de scikit-learn. http://scikit-learn.org/stable/index.html\n",
    "2. Precision y Recall. https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "3. Machine Learning 101 (Google). https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/preview?imm_mid=0f9b7e&cmp=em-data-na-na-newsltr_20171213#slide=id.g168a3288f7_0_58\n",
    "4. WEKA (un programa visual con clasificadores y otras herramientas para ML). https://www.cs.waikato.ac.nz/ml/weka/\n",
    "5. Curso de Data Mining con WEKA. https://www.cs.waikato.ac.nz/ml/weka/mooc/dataminingwithweka/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "En Random Forest se ejecutan varios algoritmos de árbol de decisiones en lugar de uno solo. Para clasificar un nuevo objeto basado en atributos, cada árbol de decisión da una clasificación y finalmente la decisión con mayor “votos” es la predicción del algoritmo.\n",
    "\n",
    "![](https://iartificial.net/wp-content/uploads/2019/06/Random-Forest-Bagging.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "data_inputRF = [\"sepalo_largo\", \"sepalo_ancho\", \"petalo_largo\", \"petalo_ancho\"]\n",
    "#target_input =[\"resultado\"]\n",
    "assemblerInputsRF = data_inputRF# + target_input\n",
    "vector_featuresRF = VectorAssembler(inputCols=assemblerInputsRF, outputCol=\"features\")\n",
    "train_RF = vector_featuresRF.transform(trainingData)\n",
    "print(train_RF)\n",
    "print(training_total.columns)\n",
    "test_RF = vector_featuresRF.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label', numTrees=10)\n",
    "rfModel = rf.fit(train_RF)\n",
    "predictionsRF = rfModel.transform(test_RF)\n",
    "predictionsRF.select('sepalo_largo', 'sepalo_ancho', 'petalo_largo', 'petalo_ancho','label', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluatorRF = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracyRF = evaluatorRF.evaluate(predictionsRF)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracyRF))\n",
    "print(\"Precisiòn = %g\" % (accuracyRF*100))\n",
    "evaluatorRF.evaluate(predictionsRF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificadores binarios\n",
    "\n",
    "Pasamos a repasar clasificadores binarios, que se pueden utilizar. Pero como en el set tenemos multiclases, el modelo debe pasar por la reduccción \"One-vs-Al\" componente \"OneVsRest\" que en base a un clasificador crea una binaria clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-Boosted tree Classifier\n",
    "\n",
    "Los clasificadores de aumento de gradiente son un grupo de algoritmos de aprendizaje automático que combinan muchos modelos de aprendizaje débiles para crear un modelo predictivo sólido. Los árboles de decisión generalmente se usan al aumentar el gradiente.\n",
    "\n",
    "![](https://2.bp.blogspot.com/-Dx97g4KSWGw/Ww0Aa8RyGUI/AAAAAAAABnc/hdEpxgQ-XforLeICrdwYisCYNJN8KuLDQCLcBGAs/s400/gradient-boosting.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "data_inputGB = [\"sepalo_largo\", \"sepalo_ancho\", \"petalo_largo\", \"petalo_ancho\"]\n",
    "#target_input =[\"resultado\"]\n",
    "assemblerInputsGB = data_inputGB# + target_input\n",
    "vector_featuresGB = VectorAssembler(inputCols=assemblerInputsGB, outputCol=\"features\")\n",
    "train_GB = vector_featuresGB.transform(trainingData)\n",
    "print(train_GB)\n",
    "print(training_total.columns)\n",
    "test_GB = vector_featuresGB.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_GB.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier, OneVsRest\n",
    "gbt = GBTClassifier(maxIter=5, maxDepth=2, featuresCol='features', labelCol=\"label\", seed=100)\n",
    "#gbt = GBTClassifier(maxIter=5, seed=100)\n",
    "ovrgbt = OneVsRest(classifier=gbt)\n",
    "print(ovrgbt.getParam)\n",
    "gbtModel = ovrgbt.fit(train_GB)\n",
    "predictionsgbt = gbtModel.transform(test_GB)\n",
    "predictionsgbt.select('sepalo_largo', 'sepalo_ancho', 'petalo_largo', 'petalo_ancho','label', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluatorgbt = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracygbt = evaluatorgbt.evaluate(predictionsgbt)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracygbt))\n",
    "print(\"Precisiòn = %g\" % (accuracygbt*100))\n",
    "evaluatorgbt.evaluate(predictionsgbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "Una máquina de vectores de soporte (SVM) es un modelo de aprendizaje automático supervisado que utiliza algoritmos de clasificación para problemas de clasificación de dos grupos. \n",
    "\n",
    "El objetivo del algoritmo de máquina de vectores de soporte es encontrar un hiperplano en un espacio N-dimensional (N - el número de características) que clasifica claramente los puntos de datos.\n",
    "\n",
    "![](https://iartificial.net/wp-content/uploads/2019/04/Clasificacion-SVM-1024x582.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "data_inputSVM = [\"sepalo_largo\", \"sepalo_ancho\", \"petalo_largo\", \"petalo_ancho\"]\n",
    "#target_input =[\"resultado\"]\n",
    "assemblerInputsSVM = data_inputSVM# + target_input\n",
    "vector_featuresSVM = VectorAssembler(inputCols=assemblerInputsSVM, outputCol=\"features\")\n",
    "train_SVM = vector_featuresSVM.transform(trainingData)\n",
    "print(train_SVM)\n",
    "print(training_total.columns)\n",
    "test_SVM = vector_featuresSVM.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "ovrSVM = OneVsRest(classifier=lsvc)\n",
    "lsvcModel = ovrSVM.fit(train_SVM)\n",
    "predictionslsvc = lsvcModel.transform(test_SVM)\n",
    "predictionslsvc.select('sepalo_largo', 'sepalo_ancho', 'petalo_largo', 'petalo_ancho','label', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluatorSVM = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracySVM = evaluatorSVM.evaluate(predictionslsvc)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracySVM))\n",
    "print(\"Precisiòn = %g\" % (accuracySVM*100))\n",
    "evaluatorSVM.evaluate(predictionslsvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba práctica 2 : Clasificación con datos del desastre del titanic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando lo aprendido en clases, se solicita lograr reponder la pregunta: `¿Podemos predecir que persona sobrevivió en el titanic?`, para ello se entrega un set de datos con 891 registros a analizar donde la columna `Survived` indica si sobrevivió (valor 1) o si no lo hizo (valor 0).\n",
    "\n",
    "Primero se debe leer bien la información entregada en el documento word que describe los distintos datos y lo que se solicita para la evaluación de la prueba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"titanic\")\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession \n",
    "sq = SQLContext(sc)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el archivo proporcionado: `la locación depende de donde se tenga en el equipo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('file:///C:/train.csv', header=True, inferSchema = True);\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datos = df.toPandas()\n",
    "datos.loc[:,'Total'] = 0\n",
    "print(datos.groupby(['Survived'],as_index=False).agg({\"Total\":\"count\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza un análisis de `correlación de variables` para ver la relación que existe entre todas las variables numéricas y principalmente con la variable de supervivencia (eliminamos la variable PassengerId del estudio, dado que no aporta información producto que es un secuancial). Para ello debemos explorar la data graficando como se muestra con el siguiente código.\n",
    "\n",
    "`En esta parte no debe agregar código sino solamente ejecutar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "print(df.dtypes)\n",
    "\n",
    "drop_list = ['PassengerId']\n",
    "\n",
    "df_c = df.select([column for column in df.columns if column not in drop_list])\n",
    "\n",
    "numeric_features = [t[0] for t in df_c.dtypes if t[1] in ('int','double')]\n",
    "df.select(numeric_features).describe().toPandas().transpose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = df.select(numeric_features).toPandas()\n",
    "axs = scatter_matrix(numeric_data, figsize=(8, 8));\n",
    "n = len(numeric_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "corr = df.select(numeric_features).toPandas().corr()\n",
    "\n",
    "sns.set(rc={\"font.style\":\"normal\",\n",
    "            \"axes.titlesize\":30,\n",
    "           \"text.color\":\"black\",\n",
    "            \"xtick.color\":\"black\",\n",
    "            \"ytick.color\":\"black\",\n",
    "            \"axes.labelcolor\":\"black\",\n",
    "            \"axes.grid\":False,\n",
    "            'axes.labelsize':30,\n",
    "            'figure.figsize':(8.0, 8.0),\n",
    "            'xtick.labelsize':20,\n",
    "            'ytick.labelsize':20})\n",
    "sns.heatmap(corr,annot = True, annot_kws={\"size\": 15},cmap=\"viridis\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora debemos revisar si en el set de datos vienen datos nulos para las variables, lo que no lleva a:\n",
    "1.- Eliminar variables del estudio si la cantidad de nulos es muy alta que no se pueda determinar un valor de reemplazo\n",
    "2.- Tratar los valores nulos de las variables, reemplazando estos valores por el valor mas ad-hoc considerando los demás datos de la variable\n",
    "\n",
    "`En esta parte no debe agregar código sino solamente ejecutar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "missing_df = df.select([c for c in df]).toPandas().isnull().sum(axis=0).reset_index()\n",
    "missing_df.columns = ['column_name', 'missing_count']\n",
    "missing_df = missing_df.loc[missing_df['missing_count']>0]\n",
    "missing_df = missing_df.sort_values(by='missing_count')\n",
    "ind = np.arange(missing_df.shape[0])\n",
    "width = 0.9\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "rects = ax.barh(ind, missing_df.missing_count.values, color='blue')\n",
    "ax.set_yticks(ind)\n",
    "ax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\n",
    "ax.set_xlabel(\"Cantidad de valores nulos\")\n",
    "ax.set_title(\"Número de valores nulos por cada variable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acuerdo a lo visto en los análisis anteriores podemos decidir que variables son necesarias para el estudio:\n",
    "`En este punto ustedes pueden determinar que variables van a utilizar, les entregaré detalles de la revisión para que puedan decidir en base a una decisión particular o la que he tomado`\n",
    "\n",
    "1.- La variable `PassengerId` es una secuencia que no nos aporta información para el estudio, sólo nos ayuda al final para identificar al pasajero que sobrevivió.\n",
    "2.- La variable `Cabin`(aunque parece muy interesante), yo la descarto dado que tiene 687 valores vacíos de 891 posibles. (Si estan interesados en completarla hay que leer mas del titanic para ver como se dividen las cabinas por clases, considerando posibles soluciones de acuerdo a estos datos).\n",
    "3.- La variable `Fare`, yo la descarto debido a que tiene fuerza en la correlación inversa con la variable `Pclass` (-0,55), pero la variable `Pclass` tiene mayor correlación inversa con la variable `Survived (que es la relación que nos interesa)` (-0,34). Por ello prefiero `Pclass` a `fare`.\n",
    "`El punto 3 es de acuerdo a cada visión, hay entrenamientos que consideran las dos variables, hay otros que sólo fare`\n",
    "\n",
    "`En este punto cada grupo puede decidir las variables a estudiar`.\n",
    "\n",
    "A continuación un estudios para las variables para completar los valores nulos de las variables `Age` y `Embarked` que las encuentro interesantes para el estudio.\n",
    "\n",
    "`En esta parte pueden agregar código para estudiar la o las variables que decidan utilizar o seguir el codigo proporcionado`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "media = df.select('Age').toPandas().mean().round(1)\n",
    "print(media)\n",
    "print(datos.groupby(['Embarked','Pclass'],as_index=False).agg({\"Total\":\"count\"}))\n",
    "print(df.where(df.Embarked.isNull()).show())\n",
    "print(df.toPandas().isnull().sum(axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el detalle de lo anterior vemos lo siguiente:\n",
    "1.- La variable `Age` tiene un promedio de 29,7 años, pero ello incluye todas las clases, puede ser una medida, pero en mi caso consideré el promedio de edad por clases (en la siguiente ejecución se reemplaza los nulos por el promedio de edad por clase). En un estudio mas avanzado podemos considerar mas variables para obtener una medida mas real para completar los valores nulos de edad.\n",
    "\n",
    "2.- La variable `Embarked`, sólo tiene dos registros con valores nulos y ambos pertenecen a primera clase, por lo que tomaremos el valor mas repetido de la variable para la primera clase que en este caso es `S`\n",
    "\n",
    "\n",
    "`En este punto ustedes pueden determinar otra manera de reemplazar los nulos o seguir el codigo proporcionado`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "prom_1 = df.select('Age').where(df.Pclass == 1).toPandas().mean().round(1)\n",
    "prom_2 = df.select('Age').where(df.Pclass == 2).toPandas().mean().round(1)\n",
    "prom_3 = df.select('Age').where(df.Pclass == 3).toPandas().mean().round(1)\n",
    "\n",
    "df_temp = df.withColumn(\"Age\",\n",
    "        when((col(\"Pclass\") == 1) & (col(\"Age\").isNull()), float(prom_1))\n",
    "        .when((col(\"Pclass\") == 2) & (col(\"Age\").isNull()), float(prom_2))\n",
    "        .when((col(\"Pclass\") == 3) & (col(\"Age\").isNull()), float(prom_3))\n",
    "        .otherwise(col(\"Age\")))\n",
    "df2 = df_temp.na.fill({'Embarked': 'S'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "missing_df = df2.select([c for c in df2]).toPandas().isnull().sum(axis=0).reset_index()\n",
    "missing_df.columns = ['column_name', 'missing_count']\n",
    "missing_df = missing_df.loc[missing_df['missing_count']>0]\n",
    "missing_df = missing_df.sort_values(by='missing_count')\n",
    "ind = np.arange(missing_df.shape[0])\n",
    "width = 0.9\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "rects = ax.barh(ind, missing_df.missing_count.values, color='blue')\n",
    "ax.set_yticks(ind)\n",
    "ax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\n",
    "ax.set_xlabel(\"Cantidad de valores nulos\")\n",
    "ax.set_title(\"Número de valores nulos por cada variable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de todo el análisis y reemplazo de valores nulos, comenzamos con la preparación de la data:\n",
    "\n",
    "1.- Eliminando las columnas que no serán parte del estudio como: `PassengerId, Name, Ticket, Fare, Cabin`\n",
    "2.- Se debe realizar el manejo de las variables string o categóricas para dejarlas en variables numéricas\n",
    "\n",
    "\n",
    "`En esta parte debe proporcionar el código para realizar el entrenamiento del clasificador`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eliminan las columnas que no se incluiran en el estudio, usted puede dejar de eliminar columnas de acuerdo a percepción de \n",
    "# las variables importantes para el estudio\n",
    "\n",
    "df2.printSchema()\n",
    "\n",
    "drop_list = ['PassengerId', 'Name','Ticket','Fare','Cabin']\n",
    "\n",
    "df_evaluar = df2.select([column for column in df.columns if column not in drop_list])\n",
    "df_evaluar.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Proporcionar el código para realizar el tratamiento de variables string a variable numérica, utilizar componentes que se \n",
    "#### muestran abajo.\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "data_string = ['Sex','Embarked']\n",
    "\n",
    "\n",
    "stages = []\n",
    "for dataCategorica in data_string:\n",
    "    stringIndexer = StringIndexer(inputCol = dataCategorica, outputCol = dataCategorica  + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[dataCategorica + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "    print(encoder.getOutputCols())\n",
    "\n",
    "data_numerica = ['Pclass','Age','SibSp','Parch']\n",
    "assemblerInputs = [c + \"classVec\" for c in data_string] + data_numerica\n",
    "vector_features = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [vector_features]\n",
    "print(assemblerInputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utilizar Pipeline para la transformación de la data y recordar renombrar la columna Survived a lavel\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df_evaluar)\n",
    "train_V = pipelineModel.transform(df_evaluar)\n",
    "train_V = (train_V\n",
    "       .withColumnRenamed('Survived','label'))\n",
    "train_V.printSchema()\n",
    "train_V.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Recordar dividir el set en entranamiento y testing e imprimir las cantidades de cada uno para corroborar la división\n",
    "\n",
    "trainingData, testData = train_V.randomSplit([0.8, 0.2], seed=100)\n",
    "print(trainingData.count())\n",
    "print(testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Entrenar el clasificador Dde árbol de decisión, recuerde elegir a juicio experto la altura \n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create initial Decision Tree Model\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=5)\n",
    "dtModel = dt.fit(trainingData)\n",
    "print(\"numNodes = \", dtModel.numNodes)\n",
    "print(\"depth = \", dtModel.depth)\n",
    "print(dtModel.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Realizar la predicción y seleccionar 10 para validar como se ve nuestra predicción\n",
    "\n",
    "predictions = dtModel.transform(testData)\n",
    "predictions.select('Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Embarked','label', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluar el clasificador, la medida que utilizaremos es la Accuracy, e imprimir la valoración\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(\"Precisiòn = %g\" % (accuracy*100))\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implementar validación cruzada (cross-validation) para ver si logramos mejora de la predicción.\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth, [3, 4, 5, 6, 7, 8, 9, 10])\n",
    "            .addGrid(dt.maxBins, [10, 20, 30, 40, 50, 60, 70, 100])\n",
    "             .build())\n",
    "\n",
    "evaluatorCV = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluatorCV, numFolds=7)\n",
    "cvModel = cv.fit(trainingData)\n",
    "print(\"numNodes = \", cvModel.bestModel.numNodes)\n",
    "print(\"depth = \", cvModel.bestModel.depth)\n",
    "print(\"depth = \", cvModel.bestModel.maxBins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Realizar las predicciones con el modelo de validación cruzada (cross-validation) y calcula el accuracy del mejor modelo\n",
    "\n",
    "predictionsCV = cvModel.transform(testData)\n",
    "evaluatorcv = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracycv = evaluatorcv.evaluate(predictionsCV)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracycv))\n",
    "print(\"Precisiòn = %g\" % (accuracycv*100))\n",
    "evaluatorcv.evaluate(predictionsCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementar gráfica de la matriz de confusión de manera de validar cuantas prediciones fueron correctas.\n",
    "### Seguir instrucciones del ejemplo realizado en clases\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_temp = predictionsCV.select(\"label\").groupBy(\"label\")\\\n",
    "                        .count().sort('count', ascending=False).toPandas()\n",
    "class_temp = class_temp[\"label\"].values.tolist()\n",
    "class_names = map(int, class_temp)\n",
    "# # # print(class_name)\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = predictionsCV.select(\"label\")\n",
    "y_true = y_true.toPandas()\n",
    "\n",
    "y_pred = predictionsCV.select(\"prediction\")\n",
    "y_pred = y_pred.toPandas()\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=list(class_names),\n",
    "                      title='Matriz de confusión')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementar dos clasificadores adicionales para poder lograr una mejor accuracy. Puede elegir alguno de los vistos en clase.\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label', numTrees=700)\n",
    "rfModel = rf.fit(trainingData)\n",
    "predictionsRF = rfModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatorRF = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracyRF = evaluatorRF.evaluate(predictionsRF)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracyRF))\n",
    "print(\"Precisiòn = %g\" % (accuracyRF*100))\n",
    "evaluatorRF.evaluate(predictionsRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier, OneVsRest\n",
    "gbt = GBTClassifier(maxIter=50)\n",
    "gbtModel = gbt.fit(trainingData)\n",
    "predictionsgbt = gbtModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluatorgbt = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracygbt = evaluatorgbt.evaluate(predictionsgbt)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracygbt))\n",
    "print(\"Precisiòn = %g\" % (accuracygbt*100))\n",
    "evaluatorgbt.evaluate(predictionsgbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=30)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictionslr = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluatorlr = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracylr = evaluatorlr.evaluate(predictionslr)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracylr))\n",
    "print(\"Precisiòn = %g\" % (accuracylr*100))\n",
    "evaluatorlr.evaluate(predictionslr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "\n",
    "1. Documentación de scikit-learn. http://scikit-learn.org/stable/index.html\n",
    "2. Precision y Recall. https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "3. Machine Learning 101 (Google). https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/preview?imm_mid=0f9b7e&cmp=em-data-na-na-newsltr_20171213#slide=id.g168a3288f7_0_58\n",
    "4. WEKA (un programa visual con clasificadores y otras herramientas para ML). https://www.cs.waikato.ac.nz/ml/weka/\n",
    "5. Curso de Data Mining con WEKA. https://www.cs.waikato.ac.nz/ml/weka/mooc/dataminingwithweka/\n",
    "6. Una buena ayuda para esta prueba es: https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
